{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students' dropout and Academic Success\n",
    "\n",
    "### Project developed by:\n",
    "- Pedro Jesus (up201907523)\n",
    "- Sofia Germer (up201907461)\n",
    "- Sérgio Estêvão (up201905680)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Specification](#Specification)\n",
    "2. [Related Work](#Related-work)\n",
    "3. [Approach](#Approach)\n",
    "4. [Required Libraries](#Required-libraries)\n",
    "5. [Data Analysis](#Data-analysis)\n",
    "6. [Data Preprocessing](#Data-Processing)\n",
    "7. Classification\n",
    "\n",
    "## Specification\n",
    "\n",
    "Our approach to this problem was as follows:\n",
    "\n",
    "1. **Data analysis:** \n",
    "2. **Algorithm implementation:**\n",
    "3. **Evalutation and refinement:**\n",
    "\n",
    "## Required Libraries\n",
    "\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "- sklearn\n",
    "- seaborn\n",
    "\n",
    "These libraries can be installed by running the following command in the terminal:\n",
    "> ``` pip install -r requirements.txt ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "We start by importing the required libraries and plotting some graphs for initial analysis of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marital status</th>\n",
       "      <th>Application mode</th>\n",
       "      <th>Application order</th>\n",
       "      <th>Course</th>\n",
       "      <th>Daytime/evening attendance\\t</th>\n",
       "      <th>Previous qualification</th>\n",
       "      <th>Previous qualification (grade)</th>\n",
       "      <th>Nacionality</th>\n",
       "      <th>Mother's qualification</th>\n",
       "      <th>Father's qualification</th>\n",
       "      <th>...</th>\n",
       "      <th>Curricular units 2nd sem (credited)</th>\n",
       "      <th>Curricular units 2nd sem (enrolled)</th>\n",
       "      <th>Curricular units 2nd sem (evaluations)</th>\n",
       "      <th>Curricular units 2nd sem (approved)</th>\n",
       "      <th>Curricular units 2nd sem (grade)</th>\n",
       "      <th>Curricular units 2nd sem (without evaluations)</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>Inflation rate</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9070</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>9773</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>8014</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Marital status  Application mode  Application order  Course  \\\n",
       "0               1                17                  5     171   \n",
       "1               1                15                  1    9254   \n",
       "2               1                 1                  5    9070   \n",
       "3               1                17                  2    9773   \n",
       "4               2                39                  1    8014   \n",
       "\n",
       "   Daytime/evening attendance\\t  Previous qualification  \\\n",
       "0                             1                       1   \n",
       "1                             1                       1   \n",
       "2                             1                       1   \n",
       "3                             1                       1   \n",
       "4                             0                       1   \n",
       "\n",
       "   Previous qualification (grade)  Nacionality  Mother's qualification  \\\n",
       "0                           122.0            1                      19   \n",
       "1                           160.0            1                       1   \n",
       "2                           122.0            1                      37   \n",
       "3                           122.0            1                      38   \n",
       "4                           100.0            1                      37   \n",
       "\n",
       "   Father's qualification  ...  Curricular units 2nd sem (credited)  \\\n",
       "0                      12  ...                                    0   \n",
       "1                       3  ...                                    0   \n",
       "2                      37  ...                                    0   \n",
       "3                      37  ...                                    0   \n",
       "4                      38  ...                                    0   \n",
       "\n",
       "   Curricular units 2nd sem (enrolled)  \\\n",
       "0                                    0   \n",
       "1                                    6   \n",
       "2                                    6   \n",
       "3                                    6   \n",
       "4                                    6   \n",
       "\n",
       "   Curricular units 2nd sem (evaluations)  \\\n",
       "0                                       0   \n",
       "1                                       6   \n",
       "2                                       0   \n",
       "3                                      10   \n",
       "4                                       6   \n",
       "\n",
       "   Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  \\\n",
       "0                                    0                          0.000000   \n",
       "1                                    6                         13.666667   \n",
       "2                                    0                          0.000000   \n",
       "3                                    5                         12.400000   \n",
       "4                                    6                         13.000000   \n",
       "\n",
       "   Curricular units 2nd sem (without evaluations)  Unemployment rate  \\\n",
       "0                                               0               10.8   \n",
       "1                                               0               13.9   \n",
       "2                                               0               10.8   \n",
       "3                                               0                9.4   \n",
       "4                                               0               13.9   \n",
       "\n",
       "   Inflation rate   GDP    Target  \n",
       "0             1.4  1.74   Dropout  \n",
       "1            -0.3  0.79  Graduate  \n",
       "2             1.4  1.74   Dropout  \n",
       "3            -0.8 -3.12  Graduate  \n",
       "4            -0.3  0.79  Graduate  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpt\n",
    "import sklearn as sk\n",
    "import seaborn as sb\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('data.csv', na_values=['NA'], delimiter=\";\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_corr = dataset.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "mask = np.zeros_like(dataset_corr)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "sb.heatmap(dataset_corr, cmap=\"YlGnBu\", annot=True, square=True, mask=mask, fmt='.2f', annot_kws={\"size\": 10});\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Data Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist(bins=30, figsize=(30, 16), sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Data Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_data = dataset.loc[dataset[\"Target\"] == \"Dropout\"]\n",
    "dropout_data.hist(bins=30, figsize=(30, 16), sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graduate Data Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduate_data = dataset.loc[dataset[\"Target\"] == \"Graduate\"]\n",
    "graduate_data.hist(bins=30, figsize=(30, 16), sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrolled Data Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enrolled_data = dataset.loc[dataset[\"Target\"] == \"Enrolled\"]\n",
    "enrolled_data.hist(bins=30, figsize=(30, 16), sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We checked the informations about the dataset and verified its consistency. After analysing the results we confirmed that there we no null values and no significant outliers, as seen in the results below, so a significant data preprocessing wasn't needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split the data into input and label sets for the SciKit classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Target'] = dataset['Target'].astype('category')\n",
    "\n",
    "col_names = list(dataset.columns)\n",
    "col_names.remove('Target')\n",
    "\n",
    "inputs = dataset[col_names].values\n",
    "labels = dataset['Target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to analyse the destribution of results and verifies a resonable class destribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "distribution_data = list(Counter(labels).values())\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "colors = ( \"#EEEEEE\",\"#999999\", \"#333333\") \n",
    "ax1.pie(distribution_data, colors = colors,labels=['Gradutated', 'Dropout', 'Enrolled'],autopct='%1.1f%%',)\n",
    "ax1.axis('equal')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used SciKit's built-in train_test_split function to generate train and test datasets. We define the training data using a 1/4 split of the entire dataset. . We use the stratify option in order to maintain the original dataset's class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(train_in,\n",
    " test_in,\n",
    " train_classes,\n",
    " test_classes) = train_test_split(inputs, labels, test_size=0.25, random_state=1, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data analysis stage showed that our working dataset was umbalanced. \n",
    "\n",
    "We implemented both undersampling and oversampling.These techniques differ in that undersampling removes samples from majority categories, while oversampling duplicates samples from minority categories. Oversampling is generally preffered, but undersampling generates smaller and therefore less complex datasets.\n",
    "\n",
    "We used random undersampling that randomly chooses which samples to remove.\n",
    "\n",
    "For oversampling we used the SMOTE (Synthetic Minority Over-sampling Technique) algorithm. This generates new samples interpolated from the existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"---Train Set---\")\n",
    "print(Counter(train_classes))\n",
    "print(\"\\n---Test Set---\")\n",
    "print(Counter(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "us_inputs, us_labels = rus.fit_resample(train_in, train_classes)\n",
    "\n",
    "print(Counter(us_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "ros = SMOTE()\n",
    "\n",
    "os_inputs, os_labels = ros.fit_resample(train_in, train_classes)\n",
    "\n",
    "print(Counter(os_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the algorithms we plan on using (KNN and SVM) require the data to be standardized. To do so, we used a StandardScaler from SciKit Learn's preprocessing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(train_in)\n",
    "train_in = scaler.fit_transform(train_in)\n",
    "test_in = scaler.fit_transform(test_in)\n",
    "\n",
    "scaler.fit(os_inputs)\n",
    "os_inputs = scaler.fit_transform(os_inputs)\n",
    "\n",
    "scaler.fit(us_inputs)\n",
    "us_inputs = scaler.fit_transform(us_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we used SciKit Learn's classification algorithm implementations to classify our data:\n",
    "\n",
    "- Decision Tree Classifier\n",
    "- Neural Networks\n",
    "- K-Nearest Neighbors\n",
    "- Support Vector Machines\n",
    "- Multilayer Perceptron (NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "### Normal Executions:\n",
    "\n",
    "- #### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "\n",
    "dtc.fit(train_in, train_classes)\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "dtc_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Original dataset ---\\n\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, dtc_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/dtc_og_report.json\", \"w\") as outfile: \n",
    "    json.dump(dtc_classification_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.fit(us_inputs, us_labels)\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "dtc_us_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Undersampled dataset ---\\n\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, dtc_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/dtc_us_report.json\", \"w\") as outfile: \n",
    "    json.dump(dtc_us_classification_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Oversampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc.fit(os_inputs, os_labels)\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "dtc_os_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Oversampled dataset ---\\n\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, dtc_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/dtc_os_report.json\", \"w\") as outfile: \n",
    "    json.dump(dtc_os_classification_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning\n",
    "\n",
    "#### GridSearchCV Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "\n",
    "parameter_grid = {'criterion': ['gini', 'entropy'],\n",
    "                  'splitter': ['best', 'random'],\n",
    "                  'max_depth': [2,4,6,8,10,12,14,16,18],\n",
    "                   'max_features':[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, \n",
    "                  31, 32, 33, 34, 35, 36, 37]}\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(),\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(train_in, train_classes)\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best estimator: {grid_search.best_estimator_}\")\n",
    "\n",
    "dtc = grid_search.best_estimator_\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "best_dtc_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Improved original dataset ---\\n\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, dtc_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/dtc_og_tunning_report.json\", \"w\") as outfile: \n",
    "    json.dump(best_dtc_classification_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(us_inputs, us_labels)\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best estimator: {grid_search.best_estimator_}\")\n",
    "\n",
    "dtc = grid_search.best_estimator_\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "best_us_dtc_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Improved undersampled dataset ---\\n\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, dtc_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/dtc_us_tunning_report.json\", \"w\") as outfile: \n",
    "    json.dump(best_us_dtc_classification_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Oversampled Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(os_inputs, os_labels)\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best estimator: {grid_search.best_estimator_}\")\n",
    "\n",
    "\n",
    "dtc = grid_search.best_estimator_\n",
    "dtc_prediction = dtc.predict(test_in)\n",
    "\n",
    "best_os_dtc_classification_report = classification_report(test_classes, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Improved oversampled dataset ---\\n\")\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, dtc_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, dtc_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/dtc_os_tunning_report.json\", \"w\") as outfile: \n",
    "    json.dump(best_os_dtc_classification_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "### Normal Executions:\n",
    "\n",
    "- #### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "mlp.fit(train_in, train_classes)\n",
    "mlp_prediction = mlp.predict(test_in)\n",
    "\n",
    "mlp_classification_report = classification_report(test_classes, mlp_prediction, output_dict=True)\n",
    "\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, mlp_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, mlp_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/mlp_og_report.json\", \"w\") as outfile: \n",
    "    json.dump(mlp_classification_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(us_inputs, us_labels)\n",
    "mlp_prediction = mlp.predict(test_in)\n",
    "\n",
    "mlp_us_classification_report = classification_report(test_classes, mlp_prediction, output_dict=True)\n",
    "\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, mlp_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, mlp_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/mlp_us_report.json\", \"w\") as outfile: \n",
    "    json.dump(mlp_us_classification_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Oversampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.fit(os_inputs, os_labels)\n",
    "mlp_prediction = mlp.predict(test_in)\n",
    "\n",
    "mlp_os_classification_report = classification_report(test_classes, mlp_prediction, output_dict=True)\n",
    "\n",
    "print(f\"Confusion matrix:\\n{confusion_matrix(test_classes, mlp_prediction)}\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(test_classes, mlp_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/mlp_os_report.json\", \"w\") as outfile: \n",
    "    json.dump(mlp_os_classification_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning\n",
    "\n",
    "#### GridSearchCV Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameter_grid = {'hidden_layer_sizes': [50, 100, 150],\n",
    "                  'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "                  'alpha': [0.0001, 0.001, 0.01], \n",
    "                  'solver': [' sgd', 'lbfgs', 'adam'],\n",
    "                  'max_iter' : [500]}\n",
    "\n",
    "grid_search_cv = GridSearchCV(MLPClassifier(),\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Original DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv.fit(train_in, train_classes)\n",
    "print(f\"Best score: {grid_search_cv.best_score_}\")\n",
    "print(f\"Best parameters: {grid_search_cv.best_params_}\")\n",
    "print(f\"Best estimator: {grid_search_cv.best_estimator_}\")\n",
    "\n",
    "nn = grid_search_cv.best_estimator_\n",
    "nn_prediction = nn.predict(train_in)\n",
    "best_report = classification_report(train_classes, nn_prediction, output_dict=True)\n",
    "\n",
    "print(\"Improved original dataset\")\n",
    "print(f\"(Confusion matrix: \\n{confusion_matrix(train_classes, nn_prediction)}\\n\\nClassification report:\\n {classification_report(train_classes, nn_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/mlp_og_tunning_report.json\", \"w\") as outfile:\n",
    "    json.dump(best_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Undersampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv.fit(us_inputs, us_labels)\n",
    "print(f\"Best score: {grid_search_cv.best_score_}\")\n",
    "print(f\"Best parameters: {grid_search_cv.best_params_}\")\n",
    "print(f\"Best estimator: {grid_search_cv.best_estimator_}\")\n",
    "\n",
    "nn = grid_search_cv.best_estimator_\n",
    "nn_prediction = nn.predict(us_inputs)\n",
    "best_report = classification_report(us_labels, nn_prediction, output_dict=True)\n",
    "\n",
    "print(\"---  ---\\n\")\n",
    "print(f\"\\n Confusion matrix: \\n{confusion_matrix(us_labels, nn_prediction)}\\n\\nClassification report:\\n {classification_report(us_labels, nn_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/mlp_us_tunning_report.json\", \"w\") as outfile:\n",
    "    json.dump(best_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Oversampled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv.fit(os_inputs, os_labels)\n",
    "print(f\"Best score: {grid_search_cv.best_score_}\")\n",
    "print(f\"Best parameters: {grid_search_cv.best_params_}\")\n",
    "print(f\"Best estimator: {grid_search_cv.best_estimator_}\")\n",
    "\n",
    "nn = grid_search_cv.best_estimator_\n",
    "nn_prediction = nn.predict(os_inputs)\n",
    "best_report = classification_report(os_labels, nn_prediction, output_dict=True)\n",
    "\n",
    "print(\"---  ---\\n\")\n",
    "print(f\"Confusion matrix: \\n{confusion_matrix(os_labels, nn_prediction)}\\n\\nClassification report:\\n {classification_report(os_labels, nn_prediction)}\\n\")\n",
    "\n",
    "with open(\"./reports/mlp_os_tunning_report.json\", \"w\") as outfile:\n",
    "    json.dump(best_report, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dar load de todos os results.json\n",
    "\n",
    "\n",
    "#DTC\n",
    "with open(\"./reports/dtc_og_report.json\") as json_file:\n",
    "    dtc_classification_report = json.load(json_file)\n",
    "with open(\"./reports/dtc_us_report.json\") as json_file:\n",
    "    dtc_us_classification_report = json.load(json_file)\n",
    "with open(\"./reports/dtc_os_report.json\") as json_file:\n",
    "    dtc_os_classification_report = json.load(json_file)\n",
    "\n",
    "with open(\"./reports/dtc_og_tunning_report.json\") as json_file:\n",
    "    dtc_tunning_classification_report = json.load(json_file)\n",
    "with open(\"./reports/dtc_us_tunning_report.json\") as json_file:\n",
    "    dtc_us_tunning_classification_report = json.load(json_file)\n",
    "with open(\"./reports/dtc_os_tunning_report.json\") as json_file:\n",
    "    dtc_os_tunning_classification_report = json.load(json_file)\n",
    "\n",
    "#NeuralNetwork\n",
    "with open(\"./reports/mlp_og_report.json\") as json_file:\n",
    "    mlp_classification_report = json.load(json_file)\n",
    "with open(\"./reports/mlp_us_report.json\") as json_file:\n",
    "    mlp_us_classification_report = json.load(json_file)\n",
    "with open(\"./reports/mlp_os_report.json\") as json_file:\n",
    "    mlp_os_classification_report = json.load(json_file)\n",
    "\n",
    "with open(\"./reports/mlp_og_tunning_report.json\") as json_file:\n",
    "    mlp_tunning_classification_report = json.load(json_file)\n",
    "with open(\"./reports/mlp_us_tunning_report.json\") as json_file:\n",
    "    mlp_us_tunning_classification_report = json.load(json_file)\n",
    "with open(\"./reports/mlp_os_tunning_report.json\") as json_file:\n",
    "    mlp_os_tunning_classification_report = json.load(json_file)\n",
    "\n",
    "#K-Nearest Neighbors\n",
    "\n",
    "#Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with remapped shapes [original->remapped]: (4,) and requested shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-fe8d303b43f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{prediction_data[2][i]:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.73\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'#6D4DA5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtick_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'KNN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SVM'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NN'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.63\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{prediction_data[3][i]:.2f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1563\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1565\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mbar\u001b[1;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[0;32m   2430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2431\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtick_labels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2432\u001b[1;33m             \u001b[0mtick_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtick_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2433\u001b[0m             \u001b[0mtick_label_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtick_label_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2434\u001b[0m             \u001b[0mtick_label_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtick_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_to\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_to\u001b[1;34m(array, shape, subok)\u001b[0m\n\u001b[0;32m    180\u001b[0m            [1, 2, 3]])\n\u001b[0;32m    181\u001b[0m     \"\"\"\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_broadcast_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\numpy\\lib\\stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_to\u001b[1;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[0;32m    123\u001b[0m                          'negative')\n\u001b[0;32m    124\u001b[0m     \u001b[0mextras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     it = np.nditer(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'multi_index'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'refs_ok'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'zerosize_ok'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextras\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         op_flags=['readonly'], itershape=shape, order='C')\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with remapped shapes [original->remapped]: (4,) and requested shape (2,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_data = [\n",
    "    [dtc_classification_report['accuracy'], mlp_classification_report['accuracy']],\n",
    "    [dtc_tunning_classification_report['accuracy'], mlp_tunning_classification_report['accuracy']],\n",
    "    [dtc_us_classification_report['accuracy'], dtc_us_classification_report['accuracy']],\n",
    "    [dtc_us_tunning_classification_report['accuracy'], mlp_us_tunning_classification_report['accuracy']],\n",
    "    [dtc_os_classification_report['accuracy'], mlp_os_classification_report['accuracy']],\n",
    "    [dtc_os_tunning_classification_report['accuracy'], mlp_os_tunning_classification_report['accuracy']],\n",
    "]\n",
    "\n",
    "X = np.arange(0,4,2)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,])\n",
    "\n",
    "ax.bar(X, prediction_data[0], color='#91232C', width=0.2)\n",
    "for i in range(2):\n",
    "    plt.text(X[i] - 0.1, prediction_data[0][i] + 0.005, f\"{prediction_data[0][i]:.2f}\")\n",
    "ax.bar(X + 0.23, prediction_data[1], color='#3C1287', width=0.2)\n",
    "for i in range(2):\n",
    "    plt.text(X[i] + 0.13, prediction_data[1][i] + 0.005, f\"{prediction_data[1][i]:.2f}\")\n",
    "\n",
    "ax.bar(X + 0.5, prediction_data[2], color='#C22F3A', width=0.2)\n",
    "for i in range(2):\n",
    "    plt.text(X[i] + 0.4, prediction_data[2][i] + 0.005, f\"{prediction_data[2][i]:.2f}\")\n",
    "ax.bar(X + 0.73, prediction_data[3], color='#6D4DA5', width=0.2, tick_label=['DT', 'KNN', 'SVM', 'NN'])\n",
    "for i in range(2):\n",
    "    plt.text(X[i] + 0.63, prediction_data[3][i] + 0.005, f\"{prediction_data[3][i]:.2f}\")\n",
    "\n",
    "ax.bar(X + 1, prediction_data[4], color='#D68C92', width=0.2)\n",
    "for i in range(2):\n",
    "    plt.text(X[i] + 0.9, prediction_data[4][i] + 0.005, f\"{prediction_data[4][i]:.2f}\")\n",
    "ax.bar(X + 1.23, prediction_data[5], color='#CEC4E1', width=0.2)\n",
    "for i in range(2):\n",
    "    plt.text(X[i] + 1.13, prediction_data[5][i] + 0.005, f\"{prediction_data[5][i]:.2f}\")\n",
    "\n",
    "ax.set_ylim(ymin=0)\n",
    "\n",
    "original = mpatches.Patch(color='#91232C', label='Original')\n",
    "best_og = mpatches.Patch(color='#3C1287', label='GS-Original')\n",
    "undersampled = mpatches.Patch(color='#C22F3A', label='Undersampling')\n",
    "best_us = mpatches.Patch(color='#6D4DA5', label='GS-Undersampling')\n",
    "oversampled = mpatches.Patch(color='#D68C92', label='Oversampling')\n",
    "best_os = mpatches.Patch(color='#CEC4E1', label='GS-Oversampling')\n",
    "plt.legend(handles=[original, best_og, undersampled, best_us, oversampled, best_os], bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "plt.xlabel(\"Algorithm\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy by algorithm\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
